from datetime import datetime
from .geospatial import get_environmental_data, Coordinates
from .clinical import assess_clinical_risk, ClinicalVitals
from .nutrition import generate_nutrition_advice
from models import RiskAssessment
import google.generativeai as genai
import os

GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')
else:
    model = None

class MatruKavachOrchestrator:
    def __init__(self):
        pass

    async def assess_mother(self, mother_id: str, name: str, lat: float, lon: float, 
                            bp_sys: int, bp_dia: int, weight: float, 
                            hb: float, glucose: int, gest_weeks: int,
                            extra_symptoms: str = None, temperature_c: float = 30.0,
                            heat_index: float = 30.0, aqi: float = 50.0, toxins: float = 2.0):
        
        from .graph import matrukavach_graph, ClinicalVitals, PlanetaryIntelligence

        initial_state = {
            "mother_id": mother_id,
            "name": name,
            "clinical_vitals": ClinicalVitals(
                systolic_bp=bp_sys,
                diastolic_bp=bp_dia,
                weight_kg=weight,
                hemoglobin=hb,
                glucose=glucose,
                gestational_age_weeks=gest_weeks,
                extra_symptoms=extra_symptoms or ""
            ),
            "planetary_intelligence": PlanetaryIntelligence(
                temperature_c=temperature_c,
                heat_index=heat_index,
                aqi=aqi,
                toxins=toxins
            )
        }

        print(f"Executing LangGraph Background Agent for {name}...")
        final_state = matrukavach_graph.invoke(initial_state)
        
        return RiskAssessment(
            mother_id=mother_id,
            overall_risk_score=round(final_state.get("final_risk_score", 1.0), 1),
            risk_level=final_state.get("risk_level", "LOW"),
            clinical_flags=final_state.get("clinical_flags", []),
            environmental_flags=final_state.get("environmental_flags", []),
            nutrition_advice=final_state.get("nutrition_advice", {}),
            medication_reminders=[], 
            environmental_impact=final_state.get("environmental_impact", ""),
            clinical_justification=final_state.get("clinical_justification", ""),
            timestamp=datetime.now()
        )

    async def generate_chat_summary(self, mother_id: str, messages: list):
        if not messages:
            return "No recent chat history found."
            
        if not model:
            return "AI model not configured for summary generation."

        formatted_messages = []
        for msg in messages:
            date_str = msg.timestamp.strftime("%Y-%m-%d %H:%M")
            sender = "Mother" if msg.sender == "Patient" else msg.sender
            text = msg.translated_text if msg.translated_text else msg.raw_text
            formatted_messages.append(f"[{date_str}] {sender}: {text}")
            
        chat_log = "\n".join(formatted_messages)
        
        prompt = f"""
        You are an AI assisting a doctor. Please provide a brief, clinical summary of the following chat history from the past two weeks between a pregnant mother and an ASHA worker/bot.
        Focus on symptoms reported, emergencies, overall tone, and any concerns raised. Keep the summary concise (1-2 paragraphs). Let it be direct.
        
        Chat History:
        {chat_log}
        """
        
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"Error generating summary: {e}")
            return "Failed to generate summary due to an AI error."
